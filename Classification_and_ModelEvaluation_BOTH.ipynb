{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GotAMinor/Training/blob/main/Classification_and_ModelEvaluation_BOTH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96XD2NrEUip"
      },
      "source": [
        "<!-- JPN -->\n",
        "# 決定木による分類とモデル評価\n",
        "\n",
        "※本演習資料の二次配布・再配布はお断り致します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzYVv_gWFEVh"
      },
      "source": [
        "<!-- ENG -->\n",
        "# Decision tree classifier and model evaluation\n",
        "\n",
        "※Distribution or redistribution of these exercise materials without the copyright holder's permission is not permitted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uKYoJWWrDGj"
      },
      "source": [
        "<!-- JPN -->\n",
        "　今回の演習の内容は以下の3つである。\n",
        "\n",
        "**1 | データクレンジング：データの整理**\n",
        "\n",
        "**2 | 決定木 (decision tree) を用いた分類**\n",
        "\n",
        "**3 | モデル選択 (model selection)**\n",
        "\n",
        "　今回から、pandas や NumPy に加えて機械学習用ライブラリである scikit-learn を用いて決定木を構築していく。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jM-FtNVFEVj"
      },
      "source": [
        "<!-- ENG -->\n",
        "　We will learn the following three things in this exercise.\n",
        "\n",
        "**1 | Data cleansing: Organizing your data**\n",
        "\n",
        "**2 | Classification using a decision tree**\n",
        "\n",
        "**3 | Model selection**\n",
        "\n",
        "　This time, we will use scikit-learn, a machine learning library, in addition to pandas and NumPy, to build decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv_jpkJu7PPe"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 1 | データクレンジング：データの整理\n",
        "\n",
        "　今回は、タイタニック号の乗客のデータ `titanic.csv` を用いて、沈没事故の際にどのような乗客が生き残ったのかを予測する。\n",
        "実はこの `titanic.csv` 、用いるデータの情報に一部抜けが存在しているため、まず**データクレンジング**と呼ばれる作業を行う（**補足資料 ※1**）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZnFFsIGFEVl"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 1 | Data cleansing: Organizing your data\n",
        "\n",
        "　Here, we will use the data of the passengers found in the Titanic, `titanic.csv`, to predict which passengers survived the sinking.\n",
        "In fact, this `titanic.csv` has some missing information in the data, so the first step is a process called **data cleansing** (**Supplementary Material S1**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0L67TrVCMkA"
      },
      "source": [
        "<!-- JPN -->\n",
        "　まず、**`titanic.csv` をGoogle Colaboratoryにアップロードする**。アップロードが終わったら、どんな情報が含まれているデータなのか見てみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04_7hvQIFEVp"
      },
      "source": [
        "<!-- ENG -->\n",
        "　First, **upload `titanic.csv` to Google Colaboratory**. After uploading is complete, let's take a look at what information the data contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIGknYHNCamq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416a9542-0cbf-42dc-f9d7-b9c8b3979a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "pd.set_option(\"display.max_columns\", None) # Setting to display all columns\n",
        "print(df.head())                           # Display only the first 5 lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7SoinkMtSe2"
      },
      "source": [
        "<!-- JPN -->\n",
        "　このデータには、生き残ったかどうかを示す`Survived`と、各乗客の情報が記されている。\n",
        "\n",
        "　今回は簡単のために、乗客の情報は`Pclass`, `Sex`, `Age`, `Fare`の4種類のみを説明変数 (explanatory variables) として使うことにして、細かく見てみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hL7uNTEFEVs"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This data includes `Survived`, which indicates whether the person survived, and information about each passenger.\n",
        "\n",
        "　For the sake of simplicity, we will use only four kinds of passenger information as explanatory variables: `Pclass`, `Sex`, `Age`, and `Fare` and look at them in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZgCKQ8JgBy4"
      },
      "outputs": [],
      "source": [
        "# Extract only the four explanatory variables and one class label\n",
        "df = df[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Survived\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVOLpa18_e14"
      },
      "source": [
        "<!-- JPN -->\n",
        "　まず、ほとんどの機械学習のモデルは説明変数や目的変数 (class label) を**数値**で受け取るので、性別を数値に変換する。このデータでは性別は2種類しかないので、0/1の2値に変換しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmDdQPkJFEVt"
      },
      "source": [
        "**加粗文字**<!-- ENG -->\n",
        "　First, since most machine learning models receive explanatory variables and class labels as **numerical values**, we need to convert gender to a numerical value. There are only two genders in this data, so let's convert it either 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3HkXI1eA8Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec97fa39-e34f-41cb-f0c9-1982f8e5d57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "# Create an array: 1 if \"male\", 0 otherwise, and assign it to X[\"IsMale\"].\n",
        "is_male = df[\"Sex\"].apply(lambda x: 1 if x == 'male' else 0) # Note that X[\"Sex\"] itself is not rewritten.\n",
        "df[\"IsMale\"] = is_male                                       # Add IsMale column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_j6KWB81pGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da57c33c-0d51-4c8c-fac8-c1c7bfd616b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Pclass     Sex   Age     Fare  Survived  IsMale\n",
            "0       3    male  22.0   7.2500         0       1\n",
            "1       1  female  38.0  71.2833         1       0\n",
            "2       3  female  26.0   7.9250         1       0\n",
            "3       1  female  35.0  53.1000         1       0\n",
            "4       3    male  35.0   8.0500         0       1\n"
          ]
        }
      ],
      "source": [
        "print(df.head()) # Make sure IsMale is set to 0 or 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgIYiDHVBQIu"
      },
      "source": [
        "<!-- JPN -->\n",
        "　次に、明らかな値の間違いがないか確認するために、年齢のヒストグラムを描画してみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OVBpZfbFEVu"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Next, let's try drawing a histogram of the ages to see if there are any obvious errors in the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Cm-tU2_eAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "87e3c319-4f69-4f42-eb10-40c03e7ce23f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df[\"Age\"].hist()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0qroBcfAQCY"
      },
      "source": [
        "<!-- JPN -->\n",
        "　最大が0才から80才程度なので、どうやら変な値は入っていないようだ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UDp1K5kFEVv"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The maximum is about in between 0 and 80 years old, so it would be expected there are no strange values in there.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNeksJ4mpuqT"
      },
      "source": [
        "<!-- JPN -->\n",
        "　次に、全ての説明変数について、値の欠損が存在しないか確認する。以下のコードで実行する `describe()` メソッドは、各数値型の特徴量の記述統計量を出力する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5Mzke7puqT"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Next, for all explanatory variables, check for the presence of missing values. The `describe()` method executed in the following code outputs the descriptive statistics of feature values of each numeric type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHngjagEAiFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9fea82-90c7-4f89-961a-9c4f9117292b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Pclass         Age        Fare    Survived      IsMale\n",
            "count  891.000000  714.000000  891.000000  891.000000  891.000000\n",
            "mean     2.308642   29.699118   32.204208    0.383838    0.647587\n",
            "std      0.836071   14.526497   49.693429    0.486592    0.477990\n",
            "min      1.000000    0.420000    0.000000    0.000000    0.000000\n",
            "25%      2.000000   20.125000    7.910400    0.000000    0.000000\n",
            "50%      3.000000   28.000000   14.454200    0.000000    1.000000\n",
            "75%      3.000000   38.000000   31.000000    1.000000    1.000000\n",
            "max      3.000000   80.000000  512.329200    1.000000    1.000000\n"
          ]
        }
      ],
      "source": [
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbD-_j2ABcr9"
      },
      "source": [
        "<!-- JPN -->\n",
        "　ここで注目すべきは、**`Age`の`count`が他の件数と異なり、714となっている**。 `describe()` メソッドでは、**欠損していないデータの数**が `count` に表示されるため、年齢情報には欠損値が存在していることを示唆している。\n",
        "\n",
        "　それでは、実際に欠損値を探してみる。値が欠損しているかどうかの判定は`isna()`で行うことができる（IS Not A number の略である）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvWkuYO0FEVv"
      },
      "source": [
        "<!-- ENG -->\n",
        "　We should pay attention to that **the `count` for the `Age` is 714**, unlike the other counts in the `describe()` method, **the number of data with missing values** is displayed in the `count`, which implies that there are missing values in the age information.\n",
        "\n",
        "　Now, let's actually try to find the missing values. The determination of whether there is a missing value can be confirmed with `isna()` (which stands for IS Not A number)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yttyWnXZCCwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce51eb98-fde6-4cae-8ffe-0b30226b2981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      False\n",
            "1      False\n",
            "2      False\n",
            "3      False\n",
            "4      False\n",
            "       ...  \n",
            "886    False\n",
            "887    False\n",
            "888     True\n",
            "889    False\n",
            "890    False\n",
            "Name: Age, Length: 891, dtype: bool\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Age\"].isna())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9UnY5vfCCB4"
      },
      "source": [
        "<!-- JPN -->\n",
        "　しばしば`True`が表示されており、値の欠損が確かに存在していることがわかる。年齢が欠損しているデータを確認してみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuzaJJhHFEVw"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Quite often `True` is displayed, indicating that a missing value is indeed present. Let's check the data that has missing values in the Age information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3Wq_spOCW9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2595f02-16f9-4219-a133-9a20fa1313ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pclass     Sex  Age     Fare  Survived  IsMale\n",
            "5         3    male  NaN   8.4583         0       1\n",
            "17        2    male  NaN  13.0000         1       1\n",
            "19        3  female  NaN   7.2250         1       0\n",
            "26        3    male  NaN   7.2250         0       1\n",
            "28        3  female  NaN   7.8792         1       0\n",
            "..      ...     ...  ...      ...       ...     ...\n",
            "859       3    male  NaN   7.2292         0       1\n",
            "863       3  female  NaN  69.5500         0       0\n",
            "868       3    male  NaN   9.5000         0       1\n",
            "878       3    male  NaN   7.8958         0       1\n",
            "888       3  female  NaN  23.4500         0       0\n",
            "\n",
            "[177 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df[df[\"Age\"].isna()]) # Extract all data that has missing values in the Age information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7gMebdiCfLI"
      },
      "source": [
        "<!-- JPN -->\n",
        "　多くの学習手法は、全てのデータが埋まりきっていることを前提としているため、欠損値を含むデータを除外する、あるいはこれらの欠損値をなんらかの値で補填する必要がある。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OdgHM9FEVw"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Many machine learning methods assume that all the data is filled in, so it is necessary to exclude data with missing values, or to compensate for these data items with some values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3FLGp5QpuqU"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "　ここでは、（極めて雑な処理なのだが）平均値で年齢を埋めてしまうことにする。欠損値を埋める、という操作は`fillna()`という関数で行うことができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf4fdN_mpuqU"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "　Here, we will fill in the age with the average (which is an extremely rough process). The operation of filling in missing values can be performed with the function `fillna()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skl2HeyVD_3O"
      },
      "outputs": [],
      "source": [
        "ave_age = df[\"Age\"].mean()\n",
        "df[\"Age\"] = df[\"Age\"].fillna(ave_age) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0YEOBACETYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdcffaf-1896-4fba-e974-bed1959ca4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Pclass         Age        Fare    Survived      IsMale\n",
            "count  891.000000  891.000000  891.000000  891.000000  891.000000\n",
            "mean     2.308642   29.699118   32.204208    0.383838    0.647587\n",
            "std      0.836071   13.002015   49.693429    0.486592    0.477990\n",
            "min      1.000000    0.420000    0.000000    0.000000    0.000000\n",
            "25%      2.000000   22.000000    7.910400    0.000000    0.000000\n",
            "50%      3.000000   29.699118   14.454200    0.000000    1.000000\n",
            "75%      3.000000   35.000000   31.000000    1.000000    1.000000\n",
            "max      3.000000   80.000000  512.329200    1.000000    1.000000\n"
          ]
        }
      ],
      "source": [
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KVessl8EVWa"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これでデータクレンジングが完了した。\n",
        "\n",
        "　最後に、説明変数 `X` と目的変数 `y` に分割した上で、`X` と `y` を**訓練データ (training data)** と**テストデータ (test data)** に分割する。これは、 scikit-learn の `train_test_split()` 関数が便利である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSxr894vFEVx"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The data cleansing is now complete.\n",
        "\n",
        "　Finally, after splitting the data into explanatory variable `X` and class label `y`, `X` and `y` are split into **training data** and **test data**. For this, scikit-learn's `train_test_split()` function is useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puWbTlZWhIjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f7bcfd-dc79-4f52-8b53-3e3869b9aedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pclass        Age     Fare  IsMale\n",
            "0         3  22.000000   7.2500       1\n",
            "1         1  38.000000  71.2833       0\n",
            "2         3  26.000000   7.9250       0\n",
            "3         1  35.000000  53.1000       0\n",
            "4         3  35.000000   8.0500       1\n",
            "..      ...        ...      ...     ...\n",
            "886       2  27.000000  13.0000       1\n",
            "887       1  19.000000  30.0000       0\n",
            "888       3  29.699118  23.4500       0\n",
            "889       1  26.000000  30.0000       1\n",
            "890       3  32.000000   7.7500       1\n",
            "\n",
            "[891 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "X = df[[\"Pclass\", \"Age\", \"Fare\", \"IsMale\"]]\n",
        "y = df[\"Survived\"]\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEwuvgUBFwIM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=0)\n",
        "# Set test_size=0.2 to make 20% of all data as test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIQmDTLPGBzt"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これにより、データは下図のように分割された。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C39gUrQeFEVy"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This resulted in the data being split as shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYv_yUuUokyo"
      },
      "source": [
        "<img src=\"https://i.imgur.com/Y1DW99k.png\" alt=\"Figure 1\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajyx2GLvo-D"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 2 | 決定木を用いた分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGIjTkubFEV1"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 2 | Classification using decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt9Yb4TO7xpc"
      },
      "source": [
        "<!-- JPN -->\n",
        "　決定木 (decision tree) とは、下図のように、**目的変数をよりキレイに分類できる説明変数と閾値で分岐させ続ける**ことで分類を行うモデルである。一般的には3つ以上にいきなり分岐させることも決定木の範疇だが、これから利用する `scikit-learn` の決定木は2つに分岐させることにのみ対応している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsVxN3CcFEV1"
      },
      "source": [
        "<!-- ENG -->\n",
        "　A decision tree is a model that performs classification by **continuously branching the class label with explanatory variables and thresholds that can classify it more neatly**, as shown in a figure. In general, although three or more branches are also within the scope of decision trees, the scikit-learn decision tree we are going to use supports only two branches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcPzdBvh-52"
      },
      "source": [
        "<img src=\"https://i.imgur.com/05tNGSr.png\" alt=\"Figure 2\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYGJwwuBiAfB"
      },
      "source": [
        "<!-- JPN -->\n",
        "**決定木の例（Wikipedia 「決定木」 より抜粋）**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWN7GSRaFEV2"
      },
      "source": [
        "<!-- ENG -->\n",
        "**Decision tree example (extracted from Wikipedia \"Decision tree\")**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZRigrPyXCSr"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.1 | `scikit-learn` を利用した決定木の構築と予測\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSpMVO-zFEV2"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.1 | Building and predicting decision trees using `scikit-learn`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbtYyr8jpuqV"
      },
      "source": [
        "<!-- JPN -->\n",
        "　決定木は `DecisionTreeClassifier()` という名前で用意されているので、これを用いる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llSzd3S3puqV"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The decision tree is provided with the name `DecisionTreeClassifier()`, which we will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF8oU4zD7EbP"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVW_tIqyCeMr"
      },
      "outputs": [],
      "source": [
        "model_full = DecisionTreeClassifier(random_state=0) # Fix random numbers for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8gzxFKyp7Ag"
      },
      "source": [
        "<!-- JPN -->\n",
        "　上記のコードは見た目上は「`DecisionTreeClassifier()` という関数に `random_state=0` という引数を渡した返り値を `model_full` に格納した」ようになっているが、実際には**決定木を構築するための（からっぽな）モデルの骨組み（インスタンス）を準備して、`model_full`という名前を付けた**と考えてほしい。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXN-ifdqFEV3"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The above code looks like \"The return value of the function `DecisionTreeClassifier()` with the argument `random_state=0` is stored in `model_full`\", however, the real meaning of the code is to **prepare the skeleton of a (empty) model (instance) for building a decision tree, and giving it the name `model_full`**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLhvXkF7puqV"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これに対して、`model_full.fit()` を行うことで、実際に決定木モデルを構築することができる。ジニ不純度 (gini impurity) を分割の基準とした決定木の構築がデフォルト値となっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8VIUrc3puqV"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Then, by performing `model_full.fit()`, we can actually build a decision tree model. Constructing a decision tree using the Gini impurity is the default option in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFXwIpwSp8ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b807b7-2085-4fb5-ae7b-9d2a2a304a48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model_full.fit(X_train, y_train) # Give the explanatory variables and class labels of the training data as arguments.\n",
        "\n",
        "# Note: The following code is wrong\n",
        "# trained = model_full.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjgacnGaM1t"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これだけで決定木の構築が完了した。 **`model_full` 自体が書き換えられるので、代入を行う必要がない**ことに注意せよ。\n",
        "\n",
        "　この学習済みモデルに対して、 `model_full.predict()` を行うことで訓練データの予測を行ってみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTX2NQseFEV3"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This completes the construction of the decision tree. Note that **the `model_full` itself is updated, so there is no need to carry out substitution**.\n",
        "\n",
        "　For this trained model, we will try to predict the training data by using `model_full.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifWHwG0o2g3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6424265-f212-4794-87b8-f6f5e47991e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "pred_y_train = model_full.predict(X_train) # Try predicting the training data\n",
        "print(pred_y_train) # Display the prediction result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27STsl2Q2nuU"
      },
      "source": [
        "<!-- JPN -->\n",
        "　出力結果は0と1の羅列だが、これは死亡=0、生存=1という意味で、今回説明変数として与えた4種類の情報から予測を行った結果を示している。\n",
        "\n",
        "　次に、実際の生存結果 `y_train` と比較を行い、予測があっていたら `True` 、間違っていたら `False` とするような配列を作成しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcQn1PXPFEV4"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The output result is a series of 0s and 1s, meaning that death=0 and survival=1, indicating that the result of prediction is based on the four types of information given as explanatory variables.\n",
        "\n",
        "　Next, let's create an array where elements will be `True` if the prediction is correct and `False` if the prediction is wrong by comparing to the actual survival result `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXishslq3BUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7e3bc8-9a64-4375-ea8b-ef2dc539ba78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140    True\n",
            "439    True\n",
            "817    True\n",
            "378    True\n",
            "491    True\n",
            "       ... \n",
            "835    True\n",
            "192    True\n",
            "629    True\n",
            "559    True\n",
            "684    True\n",
            "Name: Survived, Length: 712, dtype: bool\n"
          ]
        }
      ],
      "source": [
        "# Compare whether the results match for each element of the NumPy array\n",
        "prediction_result = (pred_y_train == y_train)\n",
        "print(prediction_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KuVzTFi3tcV"
      },
      "source": [
        "<!-- JPN -->\n",
        "　非常に `True` が多く、正しく予測できている確率が高そうだ。最後に、正解率（＝ `True` の確率）を計算する。少しトリッキーだが、`True`/`False` の配列の平均値を計算することで、正解率を求めることができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFO8iTohFEV4"
      },
      "source": [
        "<!-- ENG -->\n",
        "　There are a lot of instances where `True` is the dominant case, and the probability of predicting correctly seems to be high. Finally, calculate the accuracy (= probability of `True`). It is a little tricky, but by calculating the average value of the `True`/`False` array, you can get the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXYIMm8RaYa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dc3a60-567d-4a8b-bc78-5a21b1200073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9789325842696629\n"
          ]
        }
      ],
      "source": [
        "# Since True=1 and False=0 in np.mean,\n",
        "# the true rate can be obtained by simply calculating the mean.\n",
        "accuracy = np.mean(prediction_result)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4exDtcylRJwE"
      },
      "source": [
        "<!-- JPN -->\n",
        "　さて、決定木モデルは、学習済みモデルを画像で出力することができる。\n",
        "graphvizというツールの形式で出力されるため、`dot`というコマンドでPNG画像に変換する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRRrg2cnFEV4"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Now, the decision tree model can output the trained model as an image.\n",
        "Because the output is in the form of a tool called graphviz, convert it to a PNG image using the `dot` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHaYdhBNDBjy"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "tree.export_graphviz(model_full, out_file = \"tree_full.dot\",\n",
        "                     feature_names = X.columns,               # Display the feature value names\n",
        "                     impurity = False,                        # Simplify the display\n",
        "                     class_names = [\"Died\", \"Survive\"])       # Display the prediction results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzKhmCAoDOgx"
      },
      "outputs": [],
      "source": [
        "# Use graphviz to convert to PNG or other formats.\n",
        "# The following command is not a Python mechanism, but a Google Colaboratory mechanism\n",
        "!dot -Tpng tree_full.dot -o tree_full.png # This will create an image file\n",
        "# Download the file from the left pane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Ebcxz_Ri1V"
      },
      "source": [
        "<!-- JPN -->\n",
        "　ファイルをダウンロードして、閲覧してみよう。（とんでもない）図が作成されているはずだ。\n",
        "このように、一番上の部分から左右に2分割され続ける構造を「二分木」と言う。最初にも述べたが、`scikit-learn` の決定木は必ず二分木となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3hC8w51FEV5"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Download the file and have a look. An (outrageous) diagram should have been created.\n",
        "This structure, which is divided into two continuous parts from the top part to the left and right, is called a \"binary tree.” As mentioned at the beginning, decision trees in `scikit-learn` are always binary trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXK6VoAhi6vj"
      },
      "source": [
        "![Figure 3](https://i.imgur.com/eBhPZCO.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXH0Jp53r27p"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwOKk_jz4sil"
      },
      "source": [
        "<!-- JPN -->\n",
        "##### 課題 1\n",
        "　先ほどは訓練データ`X_train`を使って、`y_train`を予測した。\n",
        "\n",
        "　同様にテストデータ`X_test`の予測を行い、`y_test`との比較を行うことで、予測正解率を**百分率で小数点以下2桁を四捨五入し、小数点以下1桁まで**答えよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-eiC_RqFEV5"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 1\n",
        "　We used the training data `X_train` to predict `y_train` earlier.\n",
        "\n",
        "　Similarly, by predicting the test data `X_test` and comparing it with `y_test`, **round off the predicted accuracy to one decimal place as a percentage**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69Bq8vpL4sVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eeaf71a-bfcf-4f05-ce20-ede955a26798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7988826815642458\n"
          ]
        }
      ],
      "source": [
        "## Hint code\n",
        "pred_y_test = model_full.predict(X_test)\n",
        "prediction_result = (pred_y_test == y_test)\n",
        "accuracy = np.mean(prediction_result)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzYbmM4b5UOs"
      },
      "source": [
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2D95P5IXaVr"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.2 | よりシンプルな決定木の構築\n",
        "\n",
        "　2.1節で作成した決定木は非常に複雑であった。今度は、決定木の深さ（縦方向の大きさ）を制限することで、もっと簡単な決定木を作ってみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfU6TjyiFEV5"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.2 | Building a simpler decision tree\n",
        "\n",
        "　The decision tree created in section 2.1 was very complex. Now let's try to make a simpler decision tree by restricting the depth (vertical size) of the decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNqURi5mUI6i"
      },
      "source": [
        "<!-- JPN -->\n",
        "　決定木の深さはモデルの引数で制限することが可能である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFcN4rXEFEV6"
      },
      "source": [
        "<!-- ENG -->\n",
        "　The depth of the decision tree can be limited by the argument of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq1-kIgGUcvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217cf44-22a5-4b0b-8132-49a07756e222"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=3, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model_simple = DecisionTreeClassifier(max_depth=3, random_state=0) # Limit the maximum depth to 3\n",
        "model_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geOJIltib3Ug"
      },
      "source": [
        "<!-- JPN -->\n",
        "　`model_full`の時と同様に、木構造を画像にしてみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuvHz2RDFEV6"
      },
      "source": [
        "<!-- ENG -->\n",
        "　As with `model_full`, let's turn the tree structure into an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WaVbpxdNBA_"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "tree.export_graphviz(model_simple, out_file = \"tree_simple.dot\", \n",
        "                     feature_names = X.columns, \n",
        "                     impurity = False, \n",
        "                     class_names = [\"Died\", \"Survive\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUHSJ7oSNEYM"
      },
      "outputs": [],
      "source": [
        "# Use graphviz to convert to PNG, etc. graphviz is already included in Google Colab.\n",
        "!dot -Tpng tree_simple.dot -o tree_simple.png # This will create an image file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L2eQoF1O2Hk"
      },
      "source": [
        "<!-- JPN -->\n",
        "　`tree_simple.png` も画像ファイルを開いて目視してみよう。\n",
        "こちらは木の深さ（一番上から一番したまでの経路の長さ）が3に制限されているので、比較的わかりやすい図になっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM9cY0gvFEV6"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Let's also open the `tree_simple.png` and have a look at it.\n",
        "This diagram is relatively easy to understand because the depth of the tree (the length of the path from the top to the bottom) is limited to 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zUtSVpyjE3r"
      },
      "source": [
        "![Figure 4](https://i.imgur.com/mcrnEs8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWtMXnjvsBOL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPtcY-C4ayRp"
      },
      "source": [
        "<!-- JPN -->\n",
        "##### 課題 2\n",
        "\n",
        "　深さを制限した決定木を言葉で表現してみよう。例えば、一番下の段、右から4番目は「男性で、年齢が14才以下で、1等室か2等室の乗客は11人中11人生存している」という意味である。\n",
        "\n",
        "　**一番下の段、左から3番目**について、「～～で、～～～で、～～～～の乗客は92人中57人生存している」と文章で記述せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rae9dVZZFEV7"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 2\n",
        "\n",
        "　Let's try to express the depth-limited decision tree in words. For example, the bottom row, fourth from the right, means \"11 out of 11 passengers who are male, under 14 years of age, and in first or second class are alive.\"\n",
        "\n",
        "　**For the bottom row, third from the left**, write the sentence, \"57 out of 92  passengers who  are female, pclass are over 2, and fare under or equal to 23 are alive.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGNiwfABsDVh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G54XqdrnK_pt"
      },
      "source": [
        "<!-- JPN -->\n",
        "##### 課題 3\n",
        "\n",
        "　以下のコードの`__xxxxx__` 、`__yyyyy__` 、`__zzzzz__` を埋めて、確かに条件に該当する人が92人いることを確認せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQWZR5G7FEV7"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 3\n",
        "\n",
        "　Fill in `__xxxxx__`, `__yyyyy__`, and `__zzzzzzz__` for the following code to verify that there are indeed 92 people who meet the criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Yk1GqhDdzbAl",
        "outputId": "f14094a7-7f13-494f-bf0f-7ccd8823a2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pclass        Age     Fare  IsMale\n",
              "140       3  29.699118  15.2458       0\n",
              "439       2  31.000000  10.5000       1\n",
              "817       2  31.000000  37.0042       1\n",
              "378       3  20.000000   4.0125       1\n",
              "491       3  21.000000   7.2500       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5d11840-4748-4de7-8a42-b1734ed8259d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>IsMale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>3</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>15.2458</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>2</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>10.5000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>2</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>37.0042</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>3</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>4.0125</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>3</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5d11840-4748-4de7-8a42-b1734ed8259d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5d11840-4748-4de7-8a42-b1734ed8259d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5d11840-4748-4de7-8a42-b1734ed8259d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKGWGSU6LPyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9845c7-a7ff-4679-9ad1-2f050546f05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "selection = X_train[X_train['IsMale'] <= 0.5]\n",
        "selection = selection[X_train['Pclass'] > 2.5]\n",
        "selection = selection[X_train['Fare'] <= 23.35]\n",
        "print(len(selection))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paM1-_yKsFVv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOxxW5S9Jvhy"
      },
      "source": [
        "<!-- JPN -->\n",
        "##### 課題 4\n",
        "\n",
        "　`model_simple` について、訓練データ `X_train` 、`y_train` に対して予測を行った時の正解率を計算し、訓練データに対する正解率は、 `model_full` と `model_simple` どちらの方が高いか述べよ。\n",
        "\n",
        "　同様にテストデータ `X_test` 、`y_test` に対して予測を行った場合は、 `model_full` と `model_simple` どちらの方が正解率が高いか答えよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOukTwi5FEV7"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 4\n",
        "\n",
        "　For `model_simple`, calculate the accuracy of the prediction for the training data `X_train` and `y_train`, and state whether the accuracy for the training data is higher for `model_full` or `model_simple`.\n",
        "\n",
        "　In the same way, when you make predictions for the test data `X_test` and `y_test`, which has a higher accuracy, `model_full` or `model_simple`?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_pred_y_train = model_simple.predict(X_train)\n",
        "prediction_result = (sim_pred_y_train == y_train)\n",
        "accuracy = np.mean(prediction_result)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfpr8PU81y8y",
        "outputId": "e63ce764-9cee-414b-a4f9-1c5603d7e79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.827247191011236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_pred_y_test = model_simple.predict(X_test)\n",
        "prediction_result = (sim_pred_y_test == y_test)\n",
        "accuracy = np.mean(prediction_result)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVlgzDqT2fzE",
        "outputId": "d518d5fa-b69f-4a3d-83ce-a85f77ba8f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8100558659217877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_simple has a higher test accuracy."
      ],
      "metadata": {
        "id": "tRhAfQ3Y2onF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmu5AtWKsJZn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUXihhShMZz"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 2.3 | 交差検証法 (cross validation) を用いた2つのモデルの評価 (model assessment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srySaAErFEV7"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 2.3 | Assessment of two models using cross validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK9TBJIDpuqa"
      },
      "source": [
        "<!-- JPN -->\n",
        "　課題 4は、「訓練データに対する正解率」と「テストデータに対する正解率」それぞれを比較する問題であったが、どちらの方が重要だろうか。\n",
        "\n",
        "　母集団 (population) に対する平均的な予測精度（これを**汎化性能 (generalization performance)** と呼ぶ）が**高い方が利用価値の高い**モデルであるはずであり、**テストデータに対する性能で汎化性能を推定**している。\n",
        "つまり、**「テストデータに対する正解率」でモデルを比較すべきである**。\n",
        "\n",
        "　さらに、データのtraining/test分割を複数通り試すことで、**汎化性能を高精度に推定**することができる。その際に最も広く用いられるのが **$k$-分割交差検証法（$k$-fold cross validation, $k$-fold CV）**である。$k$-分割交差検証法では、データを $k$ 個に分割（図の例では $k=4$ ）して、 $k-1$ 個のグループを訓練データに、1個のグループをテストデータにした時の汎化性能を $k$ 回求め、その平均値（と標準偏差）でモデルの性能を評価する。また、$k$ がデータ数と同じ場合（つまり、一つのデータをテスト\n",
        "データにする場合）を特に**一個抜き交差検証法（leave-one-out cross validation, LOOCV）**という。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6svk8CAFpuqa"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Exercise 4 is a question about comparing “the accuracy to the training data” and “the accuracy to the test data,” but which is more important?\n",
        "\n",
        "　A model with **higher average prediction accuracy for a population (called generalization performance) should be more valuable**, so **we are estimating generalization performance by its performance on test data**.\n",
        "In other words, **models should be compared on the basis of \"accuracy to test data\".**\n",
        "\n",
        "　Furthermore, by trying multiple ways of splitting the training/test data, **the generalization performance can be estimated with high accuracy**. The most widely used method for this purpose is **$k$-fold cross validation ($k$-fold CV)**. In the $k$-fold cross validation, the data set is divided into $k$ groups ($k=4$ in the example in the figure), and the generalization performance is obtained $k$ times when $k-1$ groups are used as training data and 1 group as test data, and the performance of the model is evaluated by the mean (and standard deviation) of the results. Also, when $k$ is the same as the number of data (i.e., when one data item is used as test data), it is called **leave-one-out cross validation (LOOCV)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKuO0etjjaIl"
      },
      "source": [
        "<img src=\"https://i.imgur.com/AUMBdH0.png\" alt=\"Figure 5\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejjVns5Vjbc1"
      },
      "source": [
        "<!-- JPN -->\n",
        "**交差検証法 (cross validation) のデータ分割イメージ** | 4-fold 交差検証法を図示している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy_MsdNKFEV8"
      },
      "source": [
        "<!-- ENG -->\n",
        "**Cross validation data splitting illustration** | It illustrates the 4-fold cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-pSHQ6wVBLt"
      },
      "source": [
        "<!-- JPN -->\n",
        "　scikit-learnでは、交差検証法も `cross_val_score()` を用いることで簡単に実行することができる。関数の中でデータ分割を行うため、**`cross_val_score()` には全データを与える**ことに注意しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDJmmv1kFEV8"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In scikit-learn, cross validation can also be easily performed by using `cross_val_score()`. Note that **all the data is given to `cross_val_score()`** because data division is performed in the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYHEZDZZPHAB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "model_full = DecisionTreeClassifier(random_state=0)\n",
        "full_cv_scores = cross_val_score(model_full, X, y, cv=4)\n",
        "print(np.mean(full_cv_scores), \"+-\", np.std(full_cv_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP9GkZOQwVJw"
      },
      "outputs": [],
      "source": [
        "model_simple = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "simple_cv_scores = cross_val_score(model_simple, X, y, cv=4)\n",
        "print(np.mean(simple_cv_scores), \"+-\", np.std(simple_cv_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ytfzTl9V-eO"
      },
      "source": [
        "<!-- JPN -->\n",
        "　この結果から、木の深さを制限した決定木 `model_simple`の方が汎化性能が高いことがわかる（**補足資料 ※2**）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi-xw2WMFEV8"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This result shows that the decision tree `model_simple` with restricted tree depth has a better generalization performance (**Supplementary Material S2**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZWGO-kcDK_"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 3 | モデル選択 (model selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWm7kcbHFEV9"
      },
      "source": [
        "<!-- ENG -->\n",
        "## 3 | Model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed2As9fDpuqa"
      },
      "source": [
        "<!-- JPN -->\n",
        "　第2章で、木の深さを制限した方が汎化性能が高いことが分かったが、ここでは**汎化性能を最大化する最適な決定木の最大深さを探してみる**。このように、複数の学習モデルから最良のモデルを選ぶことを**モデル選択 (model selection)** と呼ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb9Xo138puqa"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In section 2, we found that the generalization performance is better when the depth of the tree is restricted, and here **we try to find the maximum depth of the optimal decision tree that maximizes the generalization performance**. This process of selecting the best model from multiple training models is called **model selection**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k0wn71xCl7"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 3.1 | training/validation/test 分割 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLsr6Q01FEV9"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 3.1 | training/validation/test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7AGoAspuqb"
      },
      "source": [
        "<!-- JPN -->\n",
        "　モデル選択（例：決定木における「木の最大深」の最適化）においては、汎化性能を正しく推定するために訓練データ (training data) 、**検証データ (validation data)** 、テストデータ (test data) の3データに分割した上で、以下の手順を踏む（**補足資料 ※3**）。\n",
        "\n",
        "* **モデル選択 (model selection)** ：検証データ (validation data) を最もよく予測できるモデルを選ぶ\n",
        "* **モデルの評価 (model assessment)** ：最良モデルを使ってテストデータ (test data) を予測し、汎化性能を推定する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE_EGJi3puqb"
      },
      "source": [
        "<!-- ENG -->\n",
        "In selecting the model (e.g., optimizing the \"maximum tree depth\" in decision trees), the following steps are taken after splitting the data into three, training data, **validation data**, and test data, in order to correctly estimate the generalization performance (**Supplementary Material S3**).\n",
        "\n",
        "* **Model selection**: Choose the model that best predicts the validation data\n",
        "* **Model assessment**: Predict test data using the best model and estimate generalization performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXIf9ljSYJJ0"
      },
      "source": [
        "<!-- JPN -->\n",
        "### 3.2 | 交差検証法を用いたモデルの選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0AxM9XiFEV9"
      },
      "source": [
        "<!-- ENG -->\n",
        "### 3.2 | Model selection using cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBTaypeepuqb"
      },
      "source": [
        "<!-- JPN -->\n",
        "　それでは、2.3節でも用いた交差検証法を用いて、決定木の深さの最大値はいくつにするのが最も良さそうか、確かめてみよう（このように、学習を行う前に指定しなければならない変数のことを**ハイパーパラメータ**と呼ぶ）。\n",
        "\n",
        "　この場合、**テストデータはあらかじめ除外した上で、訓練データと検証データの分割を交差検証法によって複数回試す**のが良い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLoleb8zpuqb"
      },
      "source": [
        "<!-- ENG -->\n",
        "　Let's use cross validation that we have used in section 2.3 to see what the maximum value of the depth of the decision tree should be (the variables that must be specified before training are called **hyperparameters**).\n",
        "\n",
        "　In this case, it is better to **exclude the test data beforehand and try multiple times to split the training and validation data using cross validation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ran5Upzzj_oC"
      },
      "source": [
        "<img src=\"https://i.imgur.com/2i5U90o.png\" alt=\"Figure 6\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_eYouBkBkV"
      },
      "source": [
        "<!-- JPN -->\n",
        "**モデル選択時の交差検証法 (cross validation) のデータ分割イメージ** | 20%をテストデータにしたうえで、4-fold 交差検証法を行った場合を図示している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UldwJ5HFEV9"
      },
      "source": [
        "<!-- ENG -->\n",
        "**Illustration of data splitting for cross validation when selecting the model** | The figure shows the case where 4-fold cross validation is performed after making 20% of all data set as test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx3tw45K2boL"
      },
      "source": [
        "<!-- JPN -->\n",
        "　このようなデータ分割を実現するには、まず`train_test_split()` を行い、`X_train`, `y_train`, `X_test`, `y_test`に分割した上で、`X_train`と`y_train`をつかって交差検証法を行えばよい。\n",
        "\n",
        "　ハイパーパラメータの探索時には交差検証法は極めてよく利用されるため、scikit-learnでは `GridSearchCV()` 関数が用意されている。以下では、これを用いて最適なハイパーパラメータを探索している。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRrF4pJcFEV-"
      },
      "source": [
        "<!-- ENG -->\n",
        "　To achieve the data split shown in the figure, we first perform `train_test_split()` to split the data into `X_train`, `y_train`, `X_test`, and `y_test`, and then use `X_train` and `y_train` to perform cross validation.\n",
        "\n",
        "　Since cross validation is very often used when searching for hyperparameters, it is provided as the `GridSearchCV()` function in scikit-learn. In the following, we use this to search for the optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac3dtbyc4YPe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)\n",
        "# Set test_size=0.2 to make 20% of all data as test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vF0ntxbP_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3cc9b9-4382-4467-d64d-5da3a44747e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 8}\n",
            "0.8061797752808988\n"
          ]
        }
      ],
      "source": [
        "# Array with one dictionary type in it\n",
        "param_grid = [  \n",
        "  {\n",
        "    \"max_depth\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "  }\n",
        "]\n",
        "template_model = DecisionTreeClassifier(random_state=0)          # Prepare the decision tree model framework\n",
        "grid_search_dt = GridSearchCV(template_model, param_grid, cv=4)  # Give an instruction to determine the optimal model by performing 4-fold CV.\n",
        "grid_search_dt.fit(X_train, y_train)                             # Determine the optimal model by giving the actual data and performing an internal training/validation split\n",
        "print(grid_search_dt.best_params_)                               # Confirm the optimal hyperparameters\n",
        "print(grid_search_dt.best_score_)                                # Display the prediction accuracy of the validation set on the optimal hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBMJ9_9zcYV4"
      },
      "source": [
        "<!-- JPN -->\n",
        "　これで titanic データに対する正解率が最大になる木の深さを推定することができた。新しいデータを予測する時には、このモデルを使うと良さそうだ。\n",
        "\n",
        "　最後に、このモデルでテストデータを予測することで、汎化性能を推定しよう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKc3JmsAFEV-"
      },
      "source": [
        "<!-- ENG -->\n",
        "　This will allow us to predict the maximum tree depth where the accuracy for the titanic data is at the highest. This model seems to be a good one to use when predicting new data.\n",
        "\n",
        "　Finally, let's estimate the generalization performance by predicting the test data with this model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_i6AGNz4wAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc05a684-bec6-4599-c63d-e245962fbc7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7988826815642458\n"
          ]
        }
      ],
      "source": [
        "pred_y_test = grid_search_dt.predict(X_test)\n",
        "prediction_result = (y_test == pred_y_test)\n",
        "accuracy = np.mean(prediction_result)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsI9B7CO4woX"
      },
      "source": [
        "<!-- JPN -->\n",
        "　以上で、交差検証法を用いたモデル選択を行い、最良モデルの汎化性能を推定することができた（**補足資料 ※4**）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM0xuc2cFEV_"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In the above, we were able to perform model selection using cross validation and estimate the generalization performance of the best model (**Supplementary Material S4**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6K3raw-10um"
      },
      "source": [
        "<!-- JPN -->\n",
        "　今回の授業では、2種類の交差検証法 cross validation の説明を行ったが、どう使い分ければよいか理解できただろうか。**自分が作ることのできる「学習器とハイパーパラメータの組み合わせ」が複数あるなら、訓練データと検証データを使って自分の最強モデルを決め**、**他人の手法と比較するときはテストデータを使って比較する**と考えておくと良い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDo0ZjusFEV_"
      },
      "source": [
        "<!-- ENG -->\n",
        "　In this lesson, we explained the two types of cross validations, but I hope you have understood how to use them differently. **When there are multiple \"combinations of learners and hyperparameters\" that you can create, you can use training data and validation data to determine your strongest model**, **and when comparing with other people's methods you can use test data**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9w9NlY8si5t"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JmXKwjkQ81T"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "##### 課題 5\n",
        "\n",
        "　講義資料では、決定木の最大深さのみをいろいろ変化させて評価したが、これに加えて、`min_samples_leaf`という値も同時に最適化したい。\n",
        "\n",
        "　**`min_samples_leaf`とはどういうパラメータか調べ、簡潔に答えよ**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh9I20ntFEV_"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 5\n",
        "\n",
        "　In the lecture material, we evaluated the decision tree by varying only the maximum depth of the decision tree, but we also want to optimize the value of `min_samples_leaf` at the same time.\n",
        "\n",
        "　**Find out what kind of parameter `min_samples_leaf` is, and provide a brief answer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "ztYiHZwDNZHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [  \n",
        "  {\n",
        "    ##\"max_depth\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "   'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "  }\n",
        "]\n",
        "grid_search_dt = GridSearchCV(template_model, param_grid, cv=4)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "print(grid_search_dt.best_params_)                               # Confirm the optimal hyperparameters\n",
        "print(grid_search_dt.best_score_) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sai_osgyMP5m",
        "outputId": "33baffe5-dcef-4ca7-bb02-7cff07a61073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'min_samples_leaf': 5}\n",
            "0.8047752808988764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw1kW74dapz2"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PfXfA0C9Hg2"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "##### 課題 6\n",
        "\n",
        "　次に、`GridSearchCV()` を用いて、`min_samples_leaf`も含め、パラメータ探索を行い、最良のモデルの`max_depth`と`min_samples_leaf`の値、およびテストデータ予測時の正解率を答えよ。ハイパーパラメータの探索範囲は以下の通りとして、$20 \\times 10 = 200$ 通りのハイパーパラメータから最適なパラメータを選択し、テストデータに対する正解率は**百分率で小数点以下2桁目を四捨五入し、小数点以下1桁まで**答えること。\n",
        "\n",
        "|ハイパーパラメータ名 | 値の範囲 |\n",
        "|---------------------|----------|\n",
        "|`max_depth`          |`[1,2,3, ... ,19,20]`|\n",
        "|`min_samples_leaf`   |`[1,2,3, ... ,9,10]`|\n",
        "\n",
        "なお、実行結果の再現性を担保するため、`random_state` が記述された以下のコードを使用せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx-N5iUd9Hg2"
      },
      "source": [
        "<!-- ENG -->\n",
        "##### Exercise 6\n",
        "\n",
        "Next, use `GridSearchCV()` to search for $20 \\times 10 = 200$ parameters including `min_samples_leaf`, and answer the values of `max_depth` and `min_samples_leaf` for the best model, and the percentage of the accuracy when predicting the test data. Accuracy should be given **as a percentage, rounded off to one decimal place**.\n",
        "\n",
        "|hyper parameter| value range |\n",
        "|---------------------|----------|\n",
        "|`max_depth`          |`[1,2,3, ... ,19,20]`|\n",
        "|`min_samples_leaf`   |`[1,2,3, ... ,9,10]`|\n",
        "\n",
        "To ensure reproducibility of the execution results, use the following code with `random_state` written in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7ngfLK3gM42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf64859-8025-4c6d-dd9a-9ad35329c43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 8, 'min_samples_leaf': 5}\n",
            "0.8117977528089887\n",
            "0.8212290502793296\n"
          ]
        }
      ],
      "source": [
        "# Generate data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Set search parameters\n",
        "param_grid = [\n",
        "    {\n",
        "   \"max_depth\": [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
        "   \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9,10]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Determine the best model by performing 4-fold CV\n",
        "grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=4)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "print(grid_search_dt.best_params_)                               # Confirm the optimal hyperparameters\n",
        "print(grid_search_dt.best_score_) \n",
        "\n",
        "pre_y_test = grid_search_dt.predict(X_test) # Evaluation using test data\n",
        "pre_result = [pre_y_test == y_test]\n",
        "print(np.mean(pre_result))\n",
        "# WRITE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuf4DTPHpuqd"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "##### 課題 7（実践、提出不要）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxT9hURJpuqd"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "##### Exercise 7 (Practical, not required to submit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHUqmVFXpuqd"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "決定木モデルを用いて [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download) の患者の心臓病の有無を予測してみよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRuEEAvpuqd"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "Let's predict whether a patient has heart disease or not ([Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download)) using a decision tree model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wdcy_4Fpuqd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(\"heart.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(\"target\", axis=1), data[\"target\"], test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xos9zY-puqd"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "　(1) まず、 `max_depth=4`, `min_sample_leaf=3`、それ以外のパラメータ1つを自分で設定して、決定木モデルを構築してテストデータ予測時の正解率を算出せよ。この際、テストデータに対する正解率は **百分率で小数点以下1桁** 程度にすると良い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMn6AMJtpuqd"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "　(1) First, Set `max_depth=4`, `min_sample_leaf=3` and one other parameter by yourself, build a decision tree model, predict test data, and calculate the percentage of accuracy to the test data. In this case, the percentage should be about **one decimal place in percentage**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Rhw_tRpuqd"
      },
      "source": [
        "<!-- JPN -->\n",
        "　(2) 続いて、`max_depth`, `min_sample_leaf` および (1) で選択したハイパーパラメータについてグリッドサーチを行え。グリッドサーチを行う範囲は自分で指定して良いが、最適なハイパーパラメータがその範囲の上端・下端にならないように、適宜ハイパーパラメータの範囲を調整すること。また、ハイパーパラメータごとの正解率をヒートマップ等で表示することで、ハイパーパラメータと正解率の関係性について論じよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMYwYsi_puqe"
      },
      "source": [
        "<!-- ENG -->\n",
        "　(2) Next, perform a grid search for `max_depth`, `min_sample_leaf`, and the hyperparameter selected in (1). You may specify the range of the grid search, but adjust the range of hyperparameters accordingly so that the most appropriate hyperparameters are not at the top or bottom of the range. Also, discuss the relatioship between the hyperparameters and the percentage of the accuracy using heatmaps etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHYms7lipuqe"
      },
      "source": [
        "<!-- JPN -->\n",
        "\n",
        "なお、データセットのカラムに関する説明[<sup>1</sup>](#cap1_ja)は以下の通りである。\n",
        "\n",
        "|カラム名|説明|\n",
        "|---|---|\n",
        "|age|患者の年齢|\n",
        "|sex|患者の性別|\n",
        "|cp|胸の痛みの種類 (4 種類)|\n",
        "|trestbps|安静時血圧|\n",
        "|chol|血清コレストロール(mg/dl)|\n",
        "|fbs|空腹時血糖値 (120 mg/dlより大きい場合は1, そうでない場合は0)|\n",
        "|restecg|安静時の心電図結果 (0,1,2のいずれか)|\n",
        "|thalach|最大心拍数|\n",
        "|exang|運動誘発狭心症の有無(2値)|\n",
        "|oldpeak|安静時に対して運動により誘発されるST[<sup>2</sup>](#cap2_ja)低下|\n",
        "|slope|運動時STピークセグメントの傾き|\n",
        "|ca|蛍光透視法で着色した主要血管の数(0, 1, 2, 3)|\n",
        "|thal|0 = normal; 1 = fixed defect; 2 = reversable defect|\n",
        "|target|患者の心臓病の有無 (1 = 有, 0 = 無)|\n",
        "\n",
        "<span id=\"cap1_ja\">1: [Kaggle Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download)から引用</span>  \n",
        "<span id=\"cap2_ja\">2: 心電図のS波の終わりからT波の始まりまでの部分。 心筋梗塞や心筋炎などでSTの異常な上昇がみられ、心臓からの血液の流れが悪い場合などでSTの異常な低下がみられる。\n",
        "(参考: [日本心臓財団 心臓病用語辞典](https://www.jhf.or.jp/check/term/word_a/st/)、[日本人間ドック学会 心電図](https://www.ningen-dock.jp/public/inspection/electrocardiogram))</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yebEVfjppuqe"
      },
      "source": [
        "<!-- ENG -->\n",
        "\n",
        "The explanation of the columns of the dataset is as follows.[<sup>1</sup>](#cap1_en)\n",
        "\n",
        "|column|description|\n",
        "|---|---|\n",
        "|age|age|\n",
        "|sex|sex|\n",
        "|cp|chest pain type (4 values)|\n",
        "|trestbps|resting blood pressure|\n",
        "|chol|serum cholestoral in mg/dl|\n",
        "|fbs|fasting blood sugar > 120 mg/dl|\n",
        "|restecg|resting electrocardiographic results (values 0,1,2)|\n",
        "|thalach|maximum heart rate achieved|\n",
        "|exang|exercise induced angina|\n",
        "|oldpeak|ST depression induced by exercise relative to rest|\n",
        "|slope|the slope of the peak exercise ST segment[<sup>2</sup>](#cap2_en)|\n",
        "|ca|number of major vessels (0-3) colored by flourosopy|\n",
        "|thal|0 = normal; 1 = fixed defect; 2 = reversable defect|\n",
        "|target|patient's presence of heart disease (1 = yes, 0 = no)|\n",
        "\n",
        "<span id=\"cap1_en\">1: quoted from [Kaggle Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download)</span>  \n",
        "<span id=\"cap2_en\">2: The ST segment on an electrocardiogram (ECG) normally represents an electrically neutral area of the complex between ventricular depolarization (QRS complex) and repolarization (T wave). However, it can take on various waveform morphologies that may indicate benign or clinically significant injury or insult to the myocardium. Understanding the differential diagnosis for variations in the ST segment is critical for clinical management as it can influence treatment. This article summarizes ST segment, including how it is defined, measured, and how it varies. This article also examines and summarizes ST-segment morphologies unique to various conditions that present with ST elevation or depression.\n",
        "(quoted from [National Center for Biotechnology Information, National Library of Medicine](https://www.ncbi.nlm.nih.gov/books/NBK459364/))</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1byFIoSEUnpi"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVjQVZ8z0to"
      },
      "source": [
        "<!-- JPN -->\n",
        "## レポートフォーマット\n",
        "\n",
        "提出を行う際には以下のフォーマットを利用せよ。\n",
        "\n",
        "https://docs.google.com/document/d/13ljLKSHTkBKRKj3PpeIKRAMFDgeX_PM-RWlMGz3dIXw/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTuy6MF4_qRU"
      },
      "source": [
        "<!-- ENG -->\n",
        "## Report format\n",
        "\n",
        "Use a format shown below when you submit your report.\n",
        "\n",
        "https://docs.google.com/document/d/13ljLKSHTkBKRKj3PpeIKRAMFDgeX_PM-RWlMGz3dIXw/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-aFt0WYsfl_"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 補足資料"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjVLbjt9FEWA"
      },
      "source": [
        "<!-- ENG -->\n",
        "## Supplementary Material"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1IuOlt1xA2n"
      },
      "source": [
        "<!-- JPN -->\n",
        "### ※1 データクレンジングの重要性\n",
        "\n",
        "　データサイエンスを行う場合、第2回の講義で利用したirisのように全てのデータが完全に埋まっていることはあまり多くなく、記載されている情報すら誤りが含まれている。\n",
        "\n",
        "　例えば、欠損は以下のようなときに発生する。\n",
        "* 必須登録項目ではない\n",
        "  * 例：SNSにおける生年月日\n",
        "* 選択肢に「不明」が含まれる\n",
        "  * 例：性別（LGBT等への配慮）\n",
        "\n",
        "　また、情報の誤りは以下のケースが考えられる。\n",
        "* 意図的な虚偽申告\n",
        "  * 例：1人暮らし女性などの通販サイトの「男性」としての登録（防犯上の理由）\n",
        "* 初期設定が原因である虚偽申告\n",
        "  * 都道府県（初期設定が北海道で、変更せずそのまま登録）\n",
        "* 複数人が同一アカウントを利用している\n",
        "  * 母親のアカウントで家族全員分の通販を処理\n",
        "  \n",
        "　このため、ユーザが自己申告する情報を鵜呑みにせず、購入履歴などから情報を推定することも重要になったりする。\n",
        "\n",
        "　また、機械的なものであっても、センサーの不具合や故障によって、欠測や誤った計測値の出力は発生するため、こちらもやはり前後のデータから誤りが発生していないか判定することが必要になる。\n",
        "\n",
        "　これらのデータの「汚れ」をキレイにすることは**極めて重要**であり、かつ極めて地味な作業でもある。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BBPWhWrFEWA"
      },
      "source": [
        "<!-- ENG -->\n",
        "### S1 Importance of data cleansing\n",
        "\n",
        "　In the case of data science, it is not common that all data is completely filled in like iris which was used in the second lecture, and even the information listed contains errors.\n",
        "\n",
        "　For example, missing values occur in the following cases.\n",
        "* Not a required registration field\n",
        "  * Example: Date of birth in social media platforms\n",
        "* \"Unknown\" is included as an option\n",
        "  * Example: Gender (consideration for LGBT, etc.)\n",
        "\n",
        "　In addition, the following cases of information errors are possible.\n",
        "* Intentional misrepresentation\n",
        "  * Example: Registration as \"Male\" on a mail order website for women living alone, etc. (for security reasons)\n",
        "* False declaration caused by default settings\n",
        "  * Prefecture (default setting is Hokkaido, registration applied without changing)\n",
        "* Multiple people are using the same account\n",
        "  * The parent account handles mail orders for the whole family\n",
        "  \n",
        "　For this reason, it is important not to rely solely on the information self-reported by the user, but to estimate information from the purchase history.\n",
        "\n",
        "　Even if it is mechanical, missing or incorrect measurements can be output due to sensor failure or malfunction, so it is still necessary to determine whether errors have occurred from the previous or subsequent data.\n",
        "\n",
        "　Cleaning up the \"dirty\" data on these types of data is both **extremely important** but a very tiresome task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTRmGv4W0fY"
      },
      "source": [
        "<!-- JPN -->\n",
        "### ※2 model_simpleはmodel_fullよりも「有意」に汎化性能が高いのか？\n",
        "\n",
        "　model_simpleの方がmodel_fullよりも予測精度の平均が高いことが分かったが、これは偶然なのだろうか、それとも有意に差があるのだろうか。ウィルコクソンの符号順位検定を行うことで、model_simpleがmodel_fullに勝っているかどうか統計的に評価してみよう。\n",
        "\n",
        "\n",
        "　ここでは、帰無仮説を「model_simpleの予測精度とmodel_fullの予測精度に差がない」、有意水準を $\\alpha=0.05$ とする。\n",
        "ウィルコクソンの符号順位検定は、SciPyというパッケージを利用することで行うことが出来る。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEMw4FFSFEWA"
      },
      "source": [
        "<!-- ENG -->\n",
        "### S2 Does model_simple have \"significantly\" better generalization performance than model_full?\n",
        "\n",
        "　We have found that model_simple has a higher average prediction accuracy than model_full, but is this a coincidence or is there a significant difference? Let's statistically evaluate whether model_simple outperforms model_full by performing a Wilcoxon signed-rank test.\n",
        "\n",
        "\n",
        "　Here, the null hypothesis is \"there is no difference between the prediction accuracy of model_simple and that of model_full\" and the significance level is $\\alpha=0.05$. The Wilcoxon signed-rank test can be performed by using the SciPy package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_CQW2fDYgcS"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "model_full = DecisionTreeClassifier(random_state=0)\n",
        "model_simple = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "full_cv_scores = cross_val_score(model_full, X, y, cv=4)\n",
        "simple_cv_scores = cross_val_score(model_simple, X, y, cv=4)\n",
        "\n",
        "\n",
        "print(stats.wilcoxon(full_cv_scores, simple_cv_scores, alternative=\"less\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn4Kb8cBZR7H"
      },
      "source": [
        "<!-- JPN -->\n",
        "　もし `pvalue` が 0.05 を下回っていれば帰無仮説が棄却され、model_simpleはmodel_fullに比べて有意に予測精度が高いということになる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4K39liBFEWB"
      },
      "source": [
        "<!-- ENG -->\n",
        "　If the `pvalue` is less than 0.05, it means that the null hypothesis is rejected and model_simple has significantly higher prediction accuracy than model_full..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vorR4Qem05uv"
      },
      "source": [
        "<!-- JPN -->\n",
        "### ※3 なぜ training/validation/test 分割は必要か？\n",
        "\n",
        "　講義でもtraining/validation/test分割の必要性の議論はあったと思うのだが、一応ここでも補足資料として記載しておくことにする。\n",
        "\n",
        "　こんなことを考えてみよう。世界で数学選手権が行われ、日本でその国内予選がある。競技参加者は同じ参考書を使って勉強し、国内予選に出場し、そこで最も良い成績を収めた選手1名が日本代表として世界大会で各国の代表と競う。\n",
        "\n",
        "　国内予選では問題セット $V$ が出題されたとすると、総合的な実力もさることながら、たまたま $V$ が得意な問題だった選手は良い成績を収めやすい。しかし、世界大会で出題される問題セット $T$ は問題セット $V$ とは異なる問題なので、もし難易度が全く一緒だったとしても、国内予選よりもわずかに悪い成績に落ち着く可能性が高いだろう。\n",
        "\n",
        "　関係性がわかっただろうか。**多数の国内予選出場者が多数のモデルに対応**し、予選出場者の**参考書が訓練データ**、**国内予選の問題セット $V$ が検証データ**、**世界大会の問題セット $T$ がテストデータ**である。モデル選択によってえらばれた代表モデルは**検証データを予測するのが得意だったから**選ばれた可能性があり、**汎化性能（任意の問題に対する成績の期待値）よりも高いことが多い**のである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQoEGDnBFEWB"
      },
      "source": [
        "<!-- ENG -->\n",
        "### S3 Why is the training/validation/test data splitting necessary?\n",
        "\n",
        "　Although the necessity of the training/validation/test data splitting was discussed in the lecture, I'll include it here as supplementary material.\n",
        "\n",
        "　Let's think about this. Mathematics championships are to be held all around the world, and Japan hosts its own national qualifying round. Participants in the competition study using the same reference book and compete in a preliminary round in Japan, where the best performer will represent Japan and compete against representatives from other countries in the world competition.\n",
        "\n",
        "　If question set $V$ is given at the Japanese national qualifying round, players who happen to be good at $V$, as well as their overall ability, are likely to perform well. However, question set $T$, which will be asked at the world championships, is a different question from question set $V$, so even if the difficulty levels are exactly the same, it is likely that the results will be slightly worse than in the Japanese national qualifying round.\n",
        "\n",
        "　Did you understand the relationality? **A large number of participants in the Japanese national qualifiers corresponds to a large number of models**, **the reference books of the qualifiers are the training data**, and **question set $V$ for the Japanese national qualifying rounds is the validation data**, and **question set $T$ of the world competition is the test data**. It is possible that the representative model chosen by model selection was chosen because **it was good at predicting the validation data**, which is **often higher than the generalization performance (the expected value of performance for a given question)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vLjKNfIwEQz"
      },
      "source": [
        "<!-- JPN -->\n",
        "### ※4 検証データよりもテストデータの予測精度が良くなっている？\n",
        "\n",
        "　補足資料 ※3 で議論したように、一般的にはモデル選択を行うとわずかに**検証データの方がテストデータよりも予測精度が高くなる**はずである。しかし、実際に `GridSearchCV()` を使って、検証データ、テストデータに対する予測精度をそれぞれ眺めてみると、テストデータに対する予測精度の方がわずかに高いことがある。なぜだろうか？\n",
        "\n",
        "　これは、scikit-learnの`GridSearchCV()`が、\n",
        "\n",
        "* 訓練データで学習を行いながら、最適なハイパーパラメータを探索する\n",
        "* 最適なハイパーパラメータを用いて、**訓練データ+検証データで学習を行う**\n",
        "\n",
        "という動作を行うためである。学習に用いられるデータ数が多くなることで、テストデータの予測精度が向上している可能性がある。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkfL26CUFEWB"
      },
      "source": [
        "<!-- ENG -->\n",
        "### S4 Is the prediction accuracy of the test data better than the validation data?\n",
        "\n",
        "　As discussed in Supplementary Material S3, in general, model selection should result in slightly **higher prediction accuracy for validation data than for test data**. However, if we actually use `GridSearchCV()` to look at the prediction accuracy for the validation and test data respectively, the prediction accuracy for the test data is sometimes slightly higher. Why is that?\n",
        "\n",
        "　This is because `GridSearchCV()` in scikit-learn:\n",
        "\n",
        "* Works by searching for the best hyperparameters while training on the training data and,\n",
        "* **Performs training based on training data + validation data** using optimal hyperparameters\n",
        "\n",
        "It is possible that the prediction accuracy of the test data has been improved by increasing the number of data used for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfxF0QraJYA9"
      },
      "source": [
        "<!-- JPN -->\n",
        "## 参考文献"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDbKFxIBJZf_"
      },
      "source": [
        "<!-- ENG -->\n",
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlpTRr_zJd0H"
      },
      "source": [
        "<!-- JPN -->\n",
        "- 本橋智光 著『前処理大全』（技術評論社、2018年）\n",
        "  - p.188 から、数値データの欠損値の補完について触れている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iDVs-35J-3F"
      },
      "source": [
        "<!-- ENG -->\n",
        "-  本橋智光 著『前処理大全』（技術評論社、2018年）\n",
        "  - They mentioned several strategies of filling in missing values from p.188.\n",
        "  - (Japanese book is only available)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}